{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure RAG app accuracy with Phoenix and Langflow\n",
    "\n",
    "In this notebook we'll step through how to use open source tools Langflow and Phoenix to measure the accuracy of a RAG application.\n",
    "\n",
    "## Prepping a Dataset\n",
    "\n",
    "Let's start by gathering some example data from a standard question and answer benchmark. We'll use the Standford Question Answering Dataset (SQuAD).\n",
    "\n",
    "Available at [https://rajpurkar.github.io/SQuAD-explorer/](https://rajpurkar.github.io/SQuAD-explorer/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully\n",
      "Dataset loaded with 35 articles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Download the file\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the download was successful\n",
    "if response.status_code == 200:\n",
    "    # Save the file locally\n",
    "    with open('dev-v2.0.json', 'w') as file:\n",
    "        json.dump(response.json(), file)\n",
    "    print(\"File downloaded successfully\")\n",
    "    \n",
    "    # Load the data\n",
    "    data = response.json()\n",
    "    print(f\"Dataset loaded with {len(data['data'])} articles\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all articles from the dataset\n",
    "docs = []\n",
    "for record in data[\"data\"]:\n",
    "    text = \"\"\n",
    "    for paragraph in record[\"paragraphs\"]:\n",
    "        text += paragraph[\"context\"] + \"\\n\"\n",
    "    docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all questions and answers for each article\n",
    "qandas = []\n",
    "for record in data[\"data\"]:\n",
    "    for paragraph in record[\"paragraphs\"]:\n",
    "        for qa in paragraph[\"qas\"]:\n",
    "            if len(qa[\"answers\"]) > 0:\n",
    "                qandas.append((qa[\"question\"], qa[\"answers\"][0][\"text\"]))\n",
    "\n",
    "# there are about 6000 Q&A pairs in this data set.  \n",
    "# Let's shorten to 100 to make it easier to run our tests\n",
    "import random\n",
    "samples = random.sample(qandas, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What is a ctenophora?', 'phylum of animals that live in marine waters')\n",
      "('When did Germany invade Poland and in doing so start World War II?', 'September 1939')\n",
      "('When did Honda, Toyota and Nissan open US assembly plants?', 'A decade after the 1973')\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a couple examples of questions and answers\n",
    "print(samples[0])\n",
    "print(samples[1])\n",
    "print(samples[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install modules needed for the rest of this example\n",
    "\n",
    "We'll need the following modules to be available\n",
    "\n",
    "- `langflow` for rapidly designing AI workflows\n",
    "- `phoenix-arize` to run evals\n",
    "- `pandas` to format the data\n",
    "- `openai` for access to LLMs and embedding models\n",
    "\n",
    "If you don't already have these installed, go ahead and run the install commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.5.15)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPython 3.11.7 interpreter at: \u001b[36m/opt/homebrew/opt/python@3.11/bin/python3.11\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# uv requires a virtual environment to run\n",
    "!uv venv\n",
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install all the modules for the rest of the notebook\n",
    "!uv pip install langflow pandas openai arize-phoenix -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our ground truth data to Phoenix\n",
    "\n",
    "Now that we have a set of questions and answers, we can use this to evaluate the quality of our RAG application.\n",
    "\n",
    "Let's upload this dataset to Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x31659fb90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |          | 0/1 (0.0%) | ‚è≥ 00:00<? | ?it/s\n",
      "llm_classify |          | 0/1 (0.0%) | ‚è≥ 00:00<? | ?it/s\n",
      "llm_classify |          | 0/1 (0.0%) | ‚è≥ 00:00<? | ?it/s\n",
      "llm_classify |          | 0/1 (0.0%) | ‚è≥ 00:00<? | ?it/s\n"
     ]
    }
   ],
   "source": [
    "# first make sure phoenix is running\n",
    "import phoenix as px\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by creating a dataframe with our Q&A pairs and save a copy\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(samples, columns=[\"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who increased British military resources in colonies?</td>\n",
       "      <td>William Pitt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the system by which prokaryotes retain phage gene fragments that they have previously come in contact with?</td>\n",
       "      <td>CRISPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When was most of Sunnside developed?</td>\n",
       "      <td>1950s through the 1970s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              question  \\\n",
       "0                                                                Who increased British military resources in colonies?   \n",
       "1  What is the system by which prokaryotes retain phage gene fragments that they have previously come in contact with?   \n",
       "2                                                                                 When was most of Sunnside developed?   \n",
       "\n",
       "                    answer  \n",
       "0             William Pitt  \n",
       "1                   CRISPR  \n",
       "2  1950s through the 1970s  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# or just load from the csv provided\n",
    "df = pd.read_csv(\"data/qa_pairs.csv\")\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://localhost:6006/datasets/RGF0YXNldDox/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246MQ==\n"
     ]
    }
   ],
   "source": [
    "# now let's create a dataset in Phoenix\n",
    "client = px.Client()\n",
    "dataset = client.upload_dataset(\n",
    "    dataframe=df,\n",
    "    dataset_name=\"squad-dev-v2.0\",\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[\"answer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://localhost:6006/datasets/RGF0YXNldDoy/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246Mg==\n"
     ]
    }
   ],
   "source": [
    "# for speed, let's also make a smaller dataset\n",
    "df_small = df.head(20)\n",
    "dataset = client.upload_dataset(\n",
    "    dataframe=df_small,\n",
    "    dataset_name=\"squad-dev-v2.0-x-small\",\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[\"answer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Langflow Flows from code\n",
    "\n",
    "In this section, we define a method to use the Langflow runtime to execute flows and return the results.\n",
    "\n",
    "You can find details on how to call these methods by clicking the API button in the upper right corner of the Langflow UI when a flow is open and you are editing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to run a flow in Langflow\n",
    "# trigger flow via API\n",
    "import requests\n",
    "\n",
    "BASE_API_URL = \"http://127.0.0.1:7860\"\n",
    "\n",
    "def run_flow(input: str, flow_id: str, input_type: str = \"chat\", tweaks: dict = None):\n",
    "    \"\"\" Raises exceptions for non-200 status code from langflow response\n",
    "    \"\"\"\n",
    "    api_url = f\"{BASE_API_URL}/api/v1/run/{flow_id}\"\n",
    "\n",
    "    payload = {\n",
    "        \"input_value\": input,\n",
    "        \"output_type\": \"chat\",\n",
    "        \"input_type\": input_type,\n",
    "    }\n",
    "\n",
    "    if tweaks:\n",
    "        payload[\"tweaks\"] = tweaks\n",
    "\n",
    "    timeout = 10\n",
    "    attempts = 6\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            response = requests.post(api_url, json=payload, timeout=timeout)\n",
    "            if response.status_code != 200:\n",
    "                raise requests.exceptions.RequestException(f\"Status code: {response.status_code}\")\n",
    "            return response\n",
    "        \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"The flow request timed out. Attempt {i}, current timeout {timeout}.\")\n",
    "            timeout *= 2\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load our Wikipedia data into the vector database with a flow in Langflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load_flow_id = \"d469424c-96da-44ae-b018-9b044b805144\"\n",
    "for i, doc in enumerate(docs):\n",
    "    run_flow(doc, data_load_flow_id, input_type=\"text\")\n",
    "    print(f\"Completed processing doc {i}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run an experiment with Langflow and Arize Phoenix\n",
    "\n",
    "1. Define a task to run the the a flow on the dataset\n",
    "2. Create an LLM to act as a judge\n",
    "3. Define an evaluator to measure the accuracy of the results\n",
    "4. Run the experiment and log the results to Arize Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_FLOW_ID = \"feab8feb-9f27-4132-8aa6-265962e19144\"\n",
    "\n",
    "\n",
    "def task(dataset_row) -> str:\n",
    "    question = dataset_row[\"question\"]\n",
    "    response = run_flow(question, CHAT_FLOW_ID)\n",
    "    text = response.json()['outputs'][0]['outputs'][0]['results']['message'][\"text\"]\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "\n",
    "with open(\"openai_key.json\", \"r\") as file:\n",
    "    keys = json.load(file)\n",
    "\n",
    "openai_api_key = keys.get(\"openai_api_key\")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "judge_model = OpenAIModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "from phoenix.evals import (\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: {input}\n",
      "    ************\n",
      "    [Reference]: {reference}\n",
      "    ************\n",
      "    [Answer]: {output}\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(QA_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "\n",
    "@create_evaluator(name=\"Answer Correctness\", kind=\"LLM\")\n",
    "def answer_correctness(input, output, expected) -> int:\n",
    "\n",
    "    df_in = pd.DataFrame({\n",
    "        \"input\": input[\"question\"],\n",
    "        \"output\": output,\n",
    "        \"reference\": expected[\"answer\"]\n",
    "    }, index=[0])\n",
    "              \n",
    "    rails = list(QA_PROMPT_RAILS_MAP.values())\n",
    "    \n",
    "    eval_df = llm_classify(\n",
    "        data=df_in,\n",
    "        template=QA_PROMPT_TEMPLATE,\n",
    "        model=judge_model,\n",
    "        rails=rails,\n",
    "        provide_explanation=True,\n",
    "        run_sync=True,\n",
    "    )\n",
    "\n",
    "    label = eval_df[\"label\"][0]\n",
    "    explanation = eval_df[\"explanation\"][0]\n",
    "    score = 1 if label == \"correct\" else 0\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "run_experiment(\n",
    "    dataset,\n",
    "    task=task,\n",
    "    evaluators=[answer_correctness],\n",
    "    experiment_name=\"Experiment 6 - long\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
